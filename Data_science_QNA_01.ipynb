{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI+K4JFCn2OgHm4DLyVPRY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu28vdbZ9iCe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Question:**  \n",
        "*Explain how you would handle missing data in a dataset with over 1 million records. What are the trade-offs of different imputation methods?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Deletion:** Drop rows/columns with missing values (simple but loses data).  \n",
        "- **Mean/Median Imputation:** Fast but distorts variance and relationships.  \n",
        "- **Model-Based Imputation (e.g., KNN, MICE):** More accurate but computationally expensive.  \n",
        "- **Domain-Specific Imputation:** Use business logic (e.g., filling zeros for missing sales).  \n",
        "**Trade-off:** Accuracy vs. computational cost. For large datasets, stochastic regression or approximate KNN may balance both.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Question:**  \n",
        "*How would you design an A/B test to evaluate a new recommendation algorithm? What metrics would you track, and how would you determine sample size?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Hypothesis:** Null (no difference) vs. alternative (new algorithm improves metrics).  \n",
        "- **Metrics:** Click-through rate (CTR), conversion rate, revenue per user.  \n",
        "- **Sample Size:** Use power analysis:  \n",
        "  ```python\n",
        "  from statsmodels.stats.power import TTestIndPower\n",
        "  effect_size = 0.2  # Minimum detectable effect\n",
        "  power = 0.8        # 80% power\n",
        "  alpha = 0.05       # Significance level\n",
        "  analysis = TTestIndPower()\n",
        "  sample_size = analysis.solve_power(effect_size, power=power, alpha=alpha)\n",
        "  ```  \n",
        "- **Randomization:** Split users randomly, control for confounding variables (e.g., user demographics).  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Question:**  \n",
        "*Explain the bias-variance trade-off in the context of a random forest model. How does increasing the number of trees affect this trade-off?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Bias:** Error due to overly simplistic assumptions (high bias = underfitting).  \n",
        "- **Variance:** Error due to sensitivity to small fluctuations (high variance = overfitting).  \n",
        "- **Random Forest:** Each tree has high variance but low bias (deep trees). Averaging trees reduces variance without increasing bias.  \n",
        "- **More Trees:** Decreases variance further (smoother predictions), but has diminishing returns. Bias remains unchanged.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. Question:**  \n",
        "*How would you optimize a Python script that processes a 10GB CSV file on a machine with 4GB RAM?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Chunking:** Use `pandas.read_csv(chunksize=10_000)` to process batches.  \n",
        "- **Dask/Modin:** Parallel processing libraries.  \n",
        "- **Data Types:** Reduce memory by converting floats to `float32` or categoricals.  \n",
        "- **Filter Early:** Drop unused columns/rows first.  \n",
        "- **Disk-Based Tools:** Use SQLite or DuckDB for out-of-core operations.  \n",
        "\n",
        "---\n",
        "\n",
        "### **5. Question:**  \n",
        "*Derive the gradient for logistic regression with L2 regularization. Show the impact of the regularization term.*  \n",
        "\n",
        "**Answer:**  \n",
        "Logistic loss with L2:  \n",
        "\\[\n",
        "J(\\theta) = -\\frac{1}{m} \\sum [y \\log(h_\\theta(x)) + (1-y) \\log(1-h_\\theta(x))] + \\frac{\\lambda}{2m} \\sum \\theta_j^2\n",
        "\\]  \n",
        "Gradient:  \n",
        "\\[\n",
        "\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j\n",
        "\\]  \n",
        "**Impact of λ:** Shrinks weights toward zero, preventing overfitting but may underfit if λ is too large.  \n",
        "\n",
        "---\n",
        "\n",
        "### **6. Question:**  \n",
        "*How would you explain a p-value to a non-technical stakeholder?*  \n",
        "\n",
        "**Answer:**  \n",
        "\"A p-value measures how surprising the data is if there’s no real effect. A small p-value (e.g., < 0.05) suggests the observed result (e.g., higher sales after a change) is unlikely due to random chance, so we consider it statistically significant. However, it doesn’t measure the size or importance of the effect.\"  \n",
        "\n",
        "---\n",
        "\n",
        "### **7. Question:**  \n",
        "*Write SQL to find the second-highest salary in a table `employees` with columns `id`, `name`, `salary`.*  \n",
        "\n",
        "**Answer:**  \n",
        "```sql\n",
        "SELECT DISTINCT salary\n",
        "FROM employees\n",
        "ORDER BY salary DESC\n",
        "LIMIT 1 OFFSET 1;\n",
        "```  \n",
        "*Alternative (handles ties):*  \n",
        "```sql\n",
        "SELECT MAX(salary)\n",
        "FROM employees\n",
        "WHERE salary < (SELECT MAX(salary) FROM employees);\n",
        "```  \n",
        "\n",
        "---\n",
        "\n",
        "### **8. Question:**  \n",
        "*How would you deploy a machine learning model as a REST API? Outline steps and tools.*  \n",
        "\n",
        "**Answer:**  \n",
        "1. **Serialize Model:** Save as `.pkl` (Python) or `.joblib`.  \n",
        "2. **Framework:** Use Flask/FastAPI (Python) or Django.  \n",
        "3. **API Endpoint:**  \n",
        "   ```python\n",
        "   from fastapi import FastAPI\n",
        "   import pickle\n",
        "   app = FastAPI()\n",
        "   model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
        "   @app.post(\"/predict\")\n",
        "   def predict(data: dict):\n",
        "       return {\"prediction\": model.predict([data[\"features\"]])[0]}\n",
        "   ```  \n",
        "4. **Containerize:** Docker + Kubernetes for scaling.  \n",
        "5. **Monitor:** Log predictions, track latency/errors (Prometheus, Grafana).  \n",
        "\n",
        "---\n",
        "\n",
        "### **9. Question:**  \n",
        "*Explain how a ROC curve works and how AUC is interpreted.*  \n",
        "\n",
        "**Answer:**  \n",
        "- **ROC Curve:** Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) at various thresholds.  \n",
        "- **AUC (Area Under Curve):** Measures model’s ability to distinguish classes. AUC = 0.5 (random), AUC = 1.0 (perfect).  \n",
        "- **Interpretation:** Higher AUC = better ranking of positives vs. negatives (not necessarily better calibration).  \n",
        "\n",
        "---\n",
        "\n",
        "### **10. Question:**  \n",
        "*How would you detect and handle multicollinearity in a linear regression model?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Detection:** Calculate Variance Inflation Factor (VIF):  \n",
        "  ```python\n",
        "  from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "  VIF = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "  ```  \n",
        "  VIF > 5–10 indicates high multicollinearity.  \n",
        "- **Handling:** Drop one correlated variable, use PCA, or apply regularization (Ridge/Lasso).  \n",
        "\n",
        "---\n",
        "\n",
        "### **11. Question:**  \n",
        "*Explain the difference between batch gradient descent, stochastic gradient descent (SGD), and mini-batch SGD. When would you use each?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Batch GD:** Updates weights using the entire dataset. Slow but precise (use for small datasets).  \n",
        "- **SGD:** Updates per sample. Noisy but fast (use for large datasets or online learning).  \n",
        "- **Mini-Batch SGD:** Balances both (common in deep learning; batch size 32–512).  \n",
        "\n",
        "---\n",
        "\n",
        "### **12. Question:**  \n",
        "*How would you implement a custom loss function for an imbalanced classification problem in TensorFlow?*  \n",
        "\n",
        "**Answer:**  \n",
        "```python\n",
        "def focal_loss(y_true, y_pred, alpha=0.25, gamma=2):\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "    ce = -y_true * tf.math.log(y_pred)  # Cross-entropy\n",
        "    focal = ce * alpha * (1 - y_pred) ** gamma  # Down-weight easy examples\n",
        "    return tf.reduce_mean(focal)\n",
        "```  \n",
        "*Use Case:* For class imbalance, focal loss reduces the impact of well-classified majority class examples.  \n",
        "\n",
        "---\n",
        "\n",
        "### **13. Question:**  \n",
        "*What is the curse of dimensionality? How does it affect KNN, and how can you mitigate it?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Curse:** As dimensions increase, data becomes sparse, and distance metrics lose meaning.  \n",
        "- **KNN Impact:** Distances between points converge, making neighbors irrelevant.  \n",
        "- **Mitigation:** Feature selection, PCA, or manifold learning (t-SNE, UMAP).  \n",
        "\n",
        "---\n",
        "\n",
        "### **14. Question:**  \n",
        "*Write a PySpark query to calculate the 7-day rolling average of sales per product.*  \n",
        "\n",
        "**Answer:**  \n",
        "```python\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "window = Window.partitionBy(\"product_id\").orderBy(\"date\").rowsBetween(-6, 0)\n",
        "df.withColumn(\"rolling_avg\", avg(\"sales\").over(window))\n",
        "```  \n",
        "\n",
        "---\n",
        "\n",
        "### **15. Question:**  \n",
        "*How would you troubleshoot a model that performs well on training data but poorly on validation data?*  \n",
        "\n",
        "**Answer:**  \n",
        "- **Overfitting:** Add regularization (L2/dropout), reduce model complexity, or get more data.  \n",
        "- **Data Leakage:** Ensure no validation data leaks into training (e.g., time-series splits).  \n",
        "- **Validation Set Representativeness:** Check if validation data matches real-world distribution.  \n",
        "- **Hyperparameter Tuning:** Use cross-validation to optimize parameters.  \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Lc74Zg93-p4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jg5dZ7z-uQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **How to Stand Out in Data Science Internships**  \n",
        "*(1-Page Cheat Sheet)*  \n",
        "\n",
        "#### **1. Master Core Skills**  \n",
        "- **Math/Stats:** Probability, linear algebra, hypothesis testing.  \n",
        "- **Algorithms:** Implement ML models from scratch (e.g., gradient descent).  \n",
        "- **Tools:** Python (NumPy, Pandas), SQL (window functions), Git.  \n",
        "\n",
        "#### **2. Build a Portfolio**  \n",
        "- **Projects > Certs:** End-to-end projects (cleaning → deployment).  \n",
        "- **GitHub:** Documented code + live demos (Streamlit/FastAPI).  \n",
        "- **Specialize:** Pick a niche (e.g., NLP, MLOps).  \n",
        "\n",
        "#### **3. Crush Interviews**  \n",
        "- **SQL:** CTEs, joins, optimization.  \n",
        "- **Python:** Memory efficiency, OOP.  \n",
        "- **Case Studies:** Use CRISP-DM framework.  \n",
        "\n",
        "#### **4. Learn \"Unsexy\" Tech**  \n",
        "- **Data Engineering:** Spark, Airflow, Delta Lake.  \n",
        "- **MLOps:** Model monitoring, TF Serving.  \n",
        "- **Cloud:** AWS/GCP certs (e.g., SageMaker).  \n",
        "\n",
        "#### **5. Network Strategically**  \n",
        "- **LinkedIn:** Post project breakdowns.  \n",
        "- **Meetups:** Attend PyData/NeurIPS.  \n",
        "- **Cold Outreach:** Message hiring managers with specific value propositions.  \n",
        "\n",
        "#### **6. Stay Trend-Savvy**  \n",
        "- **Papers:** Follow arXiv/Papers With Code.  \n",
        "- **Compete:** Kaggle (even if you don’t win).  \n",
        "\n",
        "#### **7. Optimize Your Resume**  \n",
        "- **Quantify Impact:** “Improved X by Y% using Z.”  \n",
        "- **Tailor:** Highlight relevant skills per role.  \n",
        "\n",
        "#### **8. Business Acumen**  \n",
        "- **Ask “Why?”:** Tie work to revenue (e.g., “Saved $2M/year”).  \n",
        "- **Domain Knowledge:** Finance? Learn risk models. Healthcare? Study HIPAA.  \n",
        "\n",
        "### **Key Mindset**  \n",
        "- **Be T-shaped:** Deep in one area, broad in others.  \n",
        "- **Show, don’t tell:** Deploy projects, write blogs.  \n",
        "- **Hustle smartly:** Network with intent, follow up.  \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7lX87Qub_V-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5Tvkbx8_faY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}